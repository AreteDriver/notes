# Decisions: January 2026

---

## ADL-20260118-001

**Project:** Gorgon
**Decision Class:** TOOLING

**Decision:** Use React + Vite + shadcn/ui for frontend stack

**Chosen Option:** React 18, Vite, TanStack Query, Zustand, shadcn/ui, Tailwind

**Rejected Options:**
- Next.js (overkill for dashboard)
- Vue/Nuxt
- Plain React with custom components

**Why:**
- Vite has fastest DX
- shadcn/ui provides consistent, accessible components
- Zustand simpler than Redux for this scale
- TanStack Query handles server state well

**Tradeoffs Accepted:**
- No SSR (acceptable for internal dashboard)
- Bundle size warning (>500KB)

**Revisit If:**
- SEO becomes requirement
- Bundle size causes UX issues

---

## ADL-20260118-002

**Project:** Knowledge Base
**Decision Class:** ARCH

**Decision:** Create shared notes repo for Claude + ARETE reference

**Chosen Option:** Structured markdown repo with topics, sessions, decisions

**Rejected Options:**
- Notion (not accessible to Claude Code)
- Project-specific CLAUDE.md files only
- No persistent documentation

**Why:**
- Both parties can reference same knowledge
- Learnings compound across sessions
- Avoids repeating same mistakes

**Tradeoffs Accepted:**
- Maintenance overhead
- Need to remember to update

**Revisit If:**
- Notes become stale and unused
- Better solution for persistent AI context emerges

---

## ADL-20260119-001

**Project:** G13_Linux
**Decision Class:** ARCH

**Decision:** Wire WebSocket handlers to daemon methods instead of inline implementation

**Chosen Option:** Server handlers call daemon.set_mode(), daemon.set_button_mapping() etc.

**Rejected Options:**
- Inline implementation in server (duplicate logic)
- Direct profile/mapper manipulation in handlers

**Why:**
- Single source of truth in daemon
- Daemon handles persistence, broadcasting, and hardware
- Server stays thin - just routes requests
- Easier to test daemon methods in isolation

**Tradeoffs Accepted:**
- Slightly more code (methods in daemon)
- Tight coupling between server and daemon

**Revisit If:**
- Need to support multiple daemons per server
- Server needs to operate independently

---

## ADL-20260124-001

**Project:** RedOPS
**Decision Class:** ARCH

**Decision:** Modular optional dependencies for AI and full feature sets

**Chosen Option:** Separate `[ai]`, `[full]`, and `[all]` optional dependency groups in pyproject.toml

**Rejected Options:**
- Include all dependencies by default (bloated install)
- Single `[extras]` group (less granular control)
- Require manual pip installs for AI

**Why:**
- Core install stays lightweight (just pydantic)
- Users only install what they need
- `pip install redops[ai]` is clear and discoverable
- CI can test minimal vs full installs separately

**Tradeoffs Accepted:**
- Users must know to install extras
- Multiple test matrices in CI

**Revisit If:**
- Too many dependency groups become confusing
- Common use cases need multiple groups combined

---

## ADL-20260124-002

**Project:** RedOPS
**Decision Class:** TOOLING

**Decision:** Use OIDC trusted publishing for PyPI instead of API tokens

**Chosen Option:** GitHub Actions OIDC with `pypa/gh-action-pypi-publish`

**Rejected Options:**
- PyPI API token in secrets
- Manual upload with twine

**Why:**
- No secrets to manage or rotate
- More secure (short-lived tokens)
- GitHub-native integration
- Industry best practice for 2025+

**Tradeoffs Accepted:**
- Requires manual setup on pypi.org
- Only works from GitHub Actions (not local)

**Revisit If:**
- Need to publish from other CI systems
- PyPI changes OIDC requirements

---

## ADL-20260125-001

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Per-provider rate limiting via semaphores for parallel AI agent execution

**Chosen Option:** `RateLimitedParallelExecutor` with asyncio semaphores per provider (Anthropic: 5, OpenAI: 8 concurrent)

**Rejected Options:**
- Global rate limiter (doesn't account for different provider limits)
- Token bucket algorithm (more complex, same outcome)
- No rate limiting (429 errors under load)

**Why:**
- Different providers have different rate limits (Anthropic ~60 RPM, OpenAI ~90 RPM)
- Semaphores are simple, native to asyncio, and predictable
- Conservative defaults (5/8) leave headroom for retries
- Provider auto-detection from task metadata reduces boilerplate

**Tradeoffs Accepted:**
- Rate limits must be tuned per deployment
- Only effective with asyncio strategy (threading falls back to parent)
- No dynamic adjustment based on 429 responses

**Revisit If:**
- Need adaptive rate limiting based on actual API responses
- Providers change rate limits significantly
- Need cross-process rate limiting (current is per-process)

---

## ADL-20260125-002

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Native async for AI providers instead of executor-wrapped sync

**Chosen Option:** `AsyncAnthropic` and `AsyncOpenAI` clients with native `complete_async()` methods

**Rejected Options:**
- Always wrap sync in `run_in_executor` (wastes thread pool)
- Sync-only API (blocks event loop)
- Third-party async wrappers

**Why:**
- Native async is more efficient (no thread overhead)
- Official SDKs support async (`AsyncAnthropic`, `AsyncOpenAI`)
- Base class provides fallback for providers without native async
- Enables proper concurrent execution with rate limiting

**Tradeoffs Accepted:**
- Must maintain both sync and async code paths
- SDK version dependency (need recent versions)

**Revisit If:**
- SDK async support becomes unreliable
- Need to support providers without async SDKs

---

## ADL-20260125-003

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Adapter pattern for incremental deprecation migration

**Chosen Option:** `WorkflowEngineAdapter` that wraps `WorkflowExecutor` with old `WorkflowEngine` interface

**Rejected Options:**
- Big-bang migration (risky, all-or-nothing)
- Maintain two parallel implementations (duplicate effort)
- Force all callers to update simultaneously (breaks existing code)

**Why:**
- Allows incremental migration - one file at a time
- Same interface means minimal code changes at call sites
- Adapter handles format conversion internally
- Deprecation warnings guide migration without breaking builds

**Tradeoffs Accepted:**
- Extra abstraction layer (minor overhead)
- Two object models coexist temporarily
- Must maintain adapter until full migration

**Revisit If:**
- All usages migrated to WorkflowExecutor (delete adapter)
- Object models diverge significantly

---

## ADL-20260126-001

**Project:** Argus Overview
**Decision Class:** SCOPE

**Decision:** Remove broadcast hotkeys feature for EVE Online EULA compliance

**Chosen Option:** Complete removal of input broadcasting functionality

**Rejected Options:**
- Keep feature with disclaimer (still violates EULA)
- Add EVE detection to disable feature (complexity, still shipped banned code)
- Make it opt-in/hidden (doesn't change EULA violation)

**Why:**
- CCP banned input broadcasting in January 2015 - permaban offense
- Feature sent keystrokes to multiple EVE windows simultaneously
- Even "fleet broadcast" framing doesn't change the technical violation
- One keypress = one action is the rule; one keypress = N actions is banned

**What Was Removed:**
- `send_key_to_window()` and `broadcast_key()` in window_capture_threaded.py
- `_register_broadcast_hotkeys()` and `_broadcast_key()` in main_window_v21.py
- Broadcast UI section in hotkeys_tab.py
- ~1159 lines of code and tests
- All documentation references

**Tradeoffs Accepted:**
- Lost feature that some users may have wanted
- Existing user settings have stale `broadcast_hotkeys` key (harmless)

**Revisit If:**
- CCP changes policy (unlikely)
- Never - this was a clear EULA violation

**Reference:**
- https://www.eveonline.com/news/view/input-broadcasting-and-multiplexing-policy
- https://support.eveonline.com/hc/en-us/articles/204873262

---

## ADL-20260127-001

**Project:** Chefwise
**Decision Class:** TOOLING

**Decision:** Migrate to Next.js 15 (not 16) as intentional stepping stone

**Chosen Option:** Next.js 15 + React 19 + ESLint 9

**Rejected Options:**
- Next.js 16 (deprecates Pages Router — requires App Router migration)
- Stay on Next.js 14 (accumulating tech debt, ESLint version conflicts)

**Why:**
- v15 fully supports Pages Router — zero code changes needed
- Resolves ESLint 8/9 version conflict that was causing CI issues
- Gets React 19 benefits (compiler improvements, performance)
- Sets up clean path for future App Router migration if needed

**Tradeoffs Accepted:**
- `next lint` deprecated in v15 (still works, needs CLI migration before v16)
- Will eventually need App Router migration for v16+

**Revisit If:**
- App Router migration becomes necessary for new features
- Next.js 16 adds must-have capabilities

---

## ADL-20260127-001

**Project:** Gorgon
**Decision Class:** SCOPE

**Decision:** Split large PRs (>2000 lines) into focused PRs by cherry-picking commits

**Chosen Option:** Close original PR, cherry-pick into 3 new branches by feature area

**Rejected Options:**
- Merge as-is (too large for meaningful review)
- Squash into single commit per branch (loses commit history)
- Rebase-based split (more complex, same result)

**Why:**
- Cherry-pick is simplest for additive-only changes (no deletions)
- Each PR independently reviewable and testable
- Clear dependency chain (PR C depends on PR B)
- Merge conflicts are manageable when both sides are purely additive

**Revisit If:**
- PRs involve deletions/renames (cherry-pick conflicts get harder)
- Branch has >10 interdependent commits

---

## ADL-20260127-002

**Project:** Gorgon
**Decision Class:** SECURITY

**Decision:** Sanitize prompt template variables by escaping curly braces before `str.format()` interpolation

**Chosen Option:** `text.replace("{", "{{").replace("}", "}}")` + 50KB length limit

**Rejected Options:**
- `string.Template` ($var syntax) — would require changing all existing templates
- Regex-based stripping — risks removing legitimate content
- No sanitization — leaves format string injection open

**Why:**
- `str.format()` interprets `{__class__}` etc. as attribute access
- Escaping is minimal, non-destructive, and backward-compatible
- Length limit prevents prompt stuffing attacks
- Applied at `PromptTemplate.format()` level — single enforcement point

**Revisit If:**
- Templates need nested formatting (double-brace escaping conflicts)
- Move to Jinja2 or another template engine

---

## ADL-20260127-003

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** Name-based dedup with platform priority for unified roster merging

**Chosen Option:** Case-insensitive name matching, ESPN > Sleeper > Yahoo priority, fill missing IDs from lower-priority sources

**Rejected Options:**
- ID-based matching across platforms (requires maintained cross-platform ID map)
- Manual-only linking (too much user friction)
- First-seen-wins (loses data from higher-quality sources)

**Why:**
- Player names are consistent enough across platforms for fantasy rosters
- ESPN has best projected points data, Sleeper has best injury data
- Merging fills gaps: one player gets ESPN ID + Sleeper ID + Yahoo ID
- Manual overrides handle the rare name mismatch

**Tradeoffs Accepted:**
- Name collisions possible (rare: "Josh Allen" QB vs "Josh Allen" DE)
- Case/accent differences could cause missed matches
- No persistent cross-platform ID mapping

**Revisit If:**
- Name collisions become a real problem (add position + team disambiguation)
- A cross-platform player ID service becomes available

---

## ADL-20260127-004

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** In-memory accuracy tracker with DB-backed decision retrieval

**Chosen Option:** `AccuracyTracker` stores outcomes in memory, queries decisions from PostgreSQL at metrics time

**Rejected Options:**
- Fully DB-backed outcomes (adds migration/schema work)
- Fully in-memory (loses data on restart)
- Redis-backed (another dependency for simple key-value)

**Why:**
- Outcomes are low-volume (users record after games end)
- Decision history already in PostgreSQL — no duplication
- In-memory is sufficient for portfolio demo
- Easy to swap to DB-backed later (just change `record_outcome` + `get_outcome`)

**Tradeoffs Accepted:**
- Outcomes lost on server restart (acceptable for demo)
- No persistence layer for outcomes yet

**Revisit If:**
- Moving to production with real users
- Need outcome data to survive deployments

---

## ADL-20260127-005

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** Sport-specific scoring functions over parameterized generic functions

**Chosen Option:** Separate `calculate_sci_mlb()`, `calculate_sci_nhl()` etc. with thin dispatchers

**Rejected Options:**
- Single parameterized function with sport-specific weight configs (flexible but opaque)
- Inheritance-based sport classes (over-engineered for pure functions)
- Config-driven scoring (YAML/JSON weight files — too indirect)

**Why:**
- Each sport has fundamentally different value drivers (OPS vs goals vs targets)
- Separate functions are readable, testable, and independently tunable
- Dispatchers are 4 lines each — negligible overhead
- Adding a new sport means adding 3 functions, not modifying shared logic

**Tradeoffs Accepted:**
- Some code duplication in RMI functions (starter/GP logic is similar across sports)
- Must update 3 dispatchers when adding a sport

**Revisit If:**
- Adding 5+ sports (consider config-driven at that point)
- Scoring logic needs user-configurable weights

---

<!-- New entries go below -->

## ADL-20260127-006

**Project:** AreteDriver Organization

**Decision Class:** INFRA

**Decision:** Enable GitHub branch protection on all private repos with `enforce_admins=false`

**Chosen Option:** Branch protection via GitHub API (`gh api repos/.../branches/main/protection`) with strict status checks and optional admin bypass

**Rejected Options:**
- No branch protection (too permissive, allows accidental direct pushes)
- Full admin enforcement (`enforce_admins=true`) — prevents emergency hotfixes without removing protection
- Require code reviews in protection — overkill for solo dev

**Why:**
- GitHub Pro unlocks branch protection on private repos (free tier doesn't)
- Strict=true enforces branch must be up-to-date before merge (no stale CI passes)
- `enforce_admins=false` is crucial for solo dev: admins can bypass in emergencies (urgent production fixes) without fully disabling protection
- Exact CI job name matching prevents flaky "checks stuck pending" states
- Batch-applied to 12 core repos provides consistency

**Implementation Details:**
- Required checks must match exact job names including matrix suffixes like `(3.10)` or `(PostgreSQL)`
- Discover job names via: `gh run view {id} -R owner/repo --json jobs --jq '.jobs[].name'`
- Settings applied: strict=true, no force pushes, no deletions, admin bypass enabled
- No require_code_reviews (adds friction for solo dev without meaningful benefit)

**Tradeoffs Accepted:**
- Workflow change: all changes now require PRs (no direct main pushes)
- Admin can bypass protection (acceptable, encourages caution but preserves emergency access)
- Must maintain accurate CI job names in protection config (when job names change, protection config must be updated)

**Revisit If:**
- Adding team members (add require_code_reviews: 1)
- Adding staging/prod environments (add deployment protection rules)
- CI job names change across repos (consider GitHub environment secrets + dynamic checks)

---

## ADL-20260127-007

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Add `in` and `not_empty` condition operators with optional `value` field

**Chosen Option:** Extend ConditionConfig with two new operators; make `value` field default to `None` so `not_empty` doesn't require it

**Rejected Options:**
- Separate ConditionConfig subclass per operator family (over-engineered for 7 operators)
- Require dummy `value` for `not_empty` (e.g., `value: true`) — misleading, adds noise to YAML

**Why:**
- Three workflow YAMLs (code-review, documentation-gen, security-audit) already used `in` operator but were blocked by validation
- `not_empty` is a natural unary check — forcing a value is semantically wrong
- Making `value` optional with `None` default is cleaner than conditional schema validation

**Tradeoffs Accepted:**
- `value: None` is valid for all operators now, not just `not_empty` — evaluate() handles gracefully but schema doesn't enforce per-operator requirements
- Four locations must stay in sync when adding operators (type literal, evaluate(), schema enum, VALID_OPERATORS set)

**Revisit If:**
- Operator count exceeds 10 (consider strategy pattern or registry)
- Need complex conditions (AND/OR/nested) — current flat model won't scale

---

## ADL-20260127-008

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Shell stdout fallback in output mapping chain: direct match → response (AI) → stdout (shell)

**Chosen Option:** Add third fallback in `_store_step_outputs` that maps `stdout` to the first custom output name for shell steps

**Rejected Options:**
- Require shell steps to always use `outputs: [stdout]` (breaks YAML readability — `code_content` is more descriptive than `stdout`)
- Auto-map all shell output keys to context (pollutes context namespace)

**Why:**
- YAML workflows should use semantic output names (`code_content`, `test_results`) not implementation details (`stdout`)
- AI steps already had response→custom name mapping; shell steps needed parity
- Single line change, no breaking changes

**Tradeoffs Accepted:**
- Only first custom output name gets the stdout mapping — subsequent names still need direct key match

**Revisit If:**
- Shell steps need multiple named outputs (e.g., stdout + stderr mapped separately)

---

## ADL-20260127-009

**Project:** AreteDriver Notes
**Decision Class:** PHIL

**Decision:** Make notes repo public after PII scrub + full history rewrite

**Chosen Option:** Redact PII with `git-filter-repo --replace-text`, then set repo to public

**Rejected Options:**
- Keep repo private (limits portfolio visibility — notes demonstrate engineering rigor)
- Delete and recreate repo (loses commit history which shows consistent practice)
- Only edit HEAD files without history rewrite (PII still visible in old commits)

**Why:**
- Public knowledge base demonstrates systematic learning and decision-making
- `git-filter-repo` rewrites all blobs — PII removed from every commit, not just HEAD
- Redacted fields (employer, city, emails, exact years) are identity-narrowing but not essential to the content's value

**Tradeoffs Accepted:**
- Force push rewrites all commit hashes (acceptable — no forks or external references)
- Must remember to scrub before adding new PII in future

**Revisit If:**
- Adding team-specific or NDA-covered content (move to private or use separate repo)

---

## ADL-20260127-010

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** Hash-based variant assignment over DB lookup for A/B testing

**Chosen Option:** SHA256 hash of session_id, bucketed by cumulative weight thresholds

**Rejected Options:**
- DB-backed assignment table (adds write on every request)
- Random assignment (non-deterministic, same user sees different variants)
- Cookie-based (requires client-side state)

**Why:**
- Zero DB writes for assignment — pure function of session_id
- Deterministic: same session always gets same variant
- Uniform distribution provable via hash properties
- No state to manage or clean up

**Tradeoffs Accepted:**
- Can't reassign a session to a different variant mid-experiment
- Changing weights mid-experiment affects new sessions only

**Revisit If:**
- Need user-level (not session-level) assignment persistence
- Need to force specific users into specific variants

---

## ADL-20260127-011

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** In-memory experiment registry with deferred DB persistence

**Chosen Option:** `ExperimentRegistry` singleton with `_active` and `_history` fields in memory

**Rejected Options:**
- DB-backed experiments table (adds schema, migrations, queries)
- File-based config (harder to manage via API)
- Redis-backed (another dependency)

**Why:**
- Experiment lifecycle is low-frequency (start/end once per experiment)
- Portfolio demo doesn't need persistence across deploys
- API endpoints work immediately, easy to add DB backing later

**Tradeoffs Accepted:**
- History lost on server restart
- Single-process only (no multi-worker consistency)

**Revisit If:**
- Moving to production with real users
- Running multiple API workers

---

<!-- New entries go below -->
