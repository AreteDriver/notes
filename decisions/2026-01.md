# Decisions: January 2026

---

## ADL-20260118-001

**Project:** Gorgon
**Decision Class:** TOOLING

**Decision:** Use React + Vite + shadcn/ui for frontend stack

**Chosen Option:** React 18, Vite, TanStack Query, Zustand, shadcn/ui, Tailwind

**Rejected Options:**
- Next.js (overkill for dashboard)
- Vue/Nuxt
- Plain React with custom components

**Why:**
- Vite has fastest DX
- shadcn/ui provides consistent, accessible components
- Zustand simpler than Redux for this scale
- TanStack Query handles server state well

**Tradeoffs Accepted:**
- No SSR (acceptable for internal dashboard)
- Bundle size warning (>500KB)

**Revisit If:**
- SEO becomes requirement
- Bundle size causes UX issues

---

## ADL-20260118-002

**Project:** Knowledge Base
**Decision Class:** ARCH

**Decision:** Create shared notes repo for Claude + ARETE reference

**Chosen Option:** Structured markdown repo with topics, sessions, decisions

**Rejected Options:**
- Notion (not accessible to Claude Code)
- Project-specific CLAUDE.md files only
- No persistent documentation

**Why:**
- Both parties can reference same knowledge
- Learnings compound across sessions
- Avoids repeating same mistakes

**Tradeoffs Accepted:**
- Maintenance overhead
- Need to remember to update

**Revisit If:**
- Notes become stale and unused
- Better solution for persistent AI context emerges

---

## ADL-20260119-001

**Project:** G13_Linux
**Decision Class:** ARCH

**Decision:** Wire WebSocket handlers to daemon methods instead of inline implementation

**Chosen Option:** Server handlers call daemon.set_mode(), daemon.set_button_mapping() etc.

**Rejected Options:**
- Inline implementation in server (duplicate logic)
- Direct profile/mapper manipulation in handlers

**Why:**
- Single source of truth in daemon
- Daemon handles persistence, broadcasting, and hardware
- Server stays thin - just routes requests
- Easier to test daemon methods in isolation

**Tradeoffs Accepted:**
- Slightly more code (methods in daemon)
- Tight coupling between server and daemon

**Revisit If:**
- Need to support multiple daemons per server
- Server needs to operate independently

---

## ADL-20260124-001

**Project:** RedOPS
**Decision Class:** ARCH

**Decision:** Modular optional dependencies for AI and full feature sets

**Chosen Option:** Separate `[ai]`, `[full]`, and `[all]` optional dependency groups in pyproject.toml

**Rejected Options:**
- Include all dependencies by default (bloated install)
- Single `[extras]` group (less granular control)
- Require manual pip installs for AI

**Why:**
- Core install stays lightweight (just pydantic)
- Users only install what they need
- `pip install redops[ai]` is clear and discoverable
- CI can test minimal vs full installs separately

**Tradeoffs Accepted:**
- Users must know to install extras
- Multiple test matrices in CI

**Revisit If:**
- Too many dependency groups become confusing
- Common use cases need multiple groups combined

---

## ADL-20260124-002

**Project:** RedOPS
**Decision Class:** TOOLING

**Decision:** Use OIDC trusted publishing for PyPI instead of API tokens

**Chosen Option:** GitHub Actions OIDC with `pypa/gh-action-pypi-publish`

**Rejected Options:**
- PyPI API token in secrets
- Manual upload with twine

**Why:**
- No secrets to manage or rotate
- More secure (short-lived tokens)
- GitHub-native integration
- Industry best practice for 2025+

**Tradeoffs Accepted:**
- Requires manual setup on pypi.org
- Only works from GitHub Actions (not local)

**Revisit If:**
- Need to publish from other CI systems
- PyPI changes OIDC requirements

---

## ADL-20260125-001

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Per-provider rate limiting via semaphores for parallel AI agent execution

**Chosen Option:** `RateLimitedParallelExecutor` with asyncio semaphores per provider (Anthropic: 5, OpenAI: 8 concurrent)

**Rejected Options:**
- Global rate limiter (doesn't account for different provider limits)
- Token bucket algorithm (more complex, same outcome)
- No rate limiting (429 errors under load)

**Why:**
- Different providers have different rate limits (Anthropic ~60 RPM, OpenAI ~90 RPM)
- Semaphores are simple, native to asyncio, and predictable
- Conservative defaults (5/8) leave headroom for retries
- Provider auto-detection from task metadata reduces boilerplate

**Tradeoffs Accepted:**
- Rate limits must be tuned per deployment
- Only effective with asyncio strategy (threading falls back to parent)
- No dynamic adjustment based on 429 responses

**Revisit If:**
- Need adaptive rate limiting based on actual API responses
- Providers change rate limits significantly
- Need cross-process rate limiting (current is per-process)

---

## ADL-20260125-002

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Native async for AI providers instead of executor-wrapped sync

**Chosen Option:** `AsyncAnthropic` and `AsyncOpenAI` clients with native `complete_async()` methods

**Rejected Options:**
- Always wrap sync in `run_in_executor` (wastes thread pool)
- Sync-only API (blocks event loop)
- Third-party async wrappers

**Why:**
- Native async is more efficient (no thread overhead)
- Official SDKs support async (`AsyncAnthropic`, `AsyncOpenAI`)
- Base class provides fallback for providers without native async
- Enables proper concurrent execution with rate limiting

**Tradeoffs Accepted:**
- Must maintain both sync and async code paths
- SDK version dependency (need recent versions)

**Revisit If:**
- SDK async support becomes unreliable
- Need to support providers without async SDKs

---

<!-- New entries go below -->
