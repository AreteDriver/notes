# Decisions: January 2026

---

## ADL-20260118-001

**Project:** Gorgon
**Decision Class:** TOOLING

**Decision:** Use React + Vite + shadcn/ui for frontend stack

**Chosen Option:** React 18, Vite, TanStack Query, Zustand, shadcn/ui, Tailwind

**Rejected Options:**
- Next.js (overkill for dashboard)
- Vue/Nuxt
- Plain React with custom components

**Why:**
- Vite has fastest DX
- shadcn/ui provides consistent, accessible components
- Zustand simpler than Redux for this scale
- TanStack Query handles server state well

**Tradeoffs Accepted:**
- No SSR (acceptable for internal dashboard)
- Bundle size warning (>500KB)

**Revisit If:**
- SEO becomes requirement
- Bundle size causes UX issues

---

## ADL-20260118-002

**Project:** Knowledge Base
**Decision Class:** ARCH

**Decision:** Create shared notes repo for Claude + ARETE reference

**Chosen Option:** Structured markdown repo with topics, sessions, decisions

**Rejected Options:**
- Notion (not accessible to Claude Code)
- Project-specific CLAUDE.md files only
- No persistent documentation

**Why:**
- Both parties can reference same knowledge
- Learnings compound across sessions
- Avoids repeating same mistakes

**Tradeoffs Accepted:**
- Maintenance overhead
- Need to remember to update

**Revisit If:**
- Notes become stale and unused
- Better solution for persistent AI context emerges

---

## ADL-20260119-001

**Project:** G13_Linux
**Decision Class:** ARCH

**Decision:** Wire WebSocket handlers to daemon methods instead of inline implementation

**Chosen Option:** Server handlers call daemon.set_mode(), daemon.set_button_mapping() etc.

**Rejected Options:**
- Inline implementation in server (duplicate logic)
- Direct profile/mapper manipulation in handlers

**Why:**
- Single source of truth in daemon
- Daemon handles persistence, broadcasting, and hardware
- Server stays thin - just routes requests
- Easier to test daemon methods in isolation

**Tradeoffs Accepted:**
- Slightly more code (methods in daemon)
- Tight coupling between server and daemon

**Revisit If:**
- Need to support multiple daemons per server
- Server needs to operate independently

---

## ADL-20260124-001

**Project:** RedOPS
**Decision Class:** ARCH

**Decision:** Modular optional dependencies for AI and full feature sets

**Chosen Option:** Separate `[ai]`, `[full]`, and `[all]` optional dependency groups in pyproject.toml

**Rejected Options:**
- Include all dependencies by default (bloated install)
- Single `[extras]` group (less granular control)
- Require manual pip installs for AI

**Why:**
- Core install stays lightweight (just pydantic)
- Users only install what they need
- `pip install redops[ai]` is clear and discoverable
- CI can test minimal vs full installs separately

**Tradeoffs Accepted:**
- Users must know to install extras
- Multiple test matrices in CI

**Revisit If:**
- Too many dependency groups become confusing
- Common use cases need multiple groups combined

---

## ADL-20260124-002

**Project:** RedOPS
**Decision Class:** TOOLING

**Decision:** Use OIDC trusted publishing for PyPI instead of API tokens

**Chosen Option:** GitHub Actions OIDC with `pypa/gh-action-pypi-publish`

**Rejected Options:**
- PyPI API token in secrets
- Manual upload with twine

**Why:**
- No secrets to manage or rotate
- More secure (short-lived tokens)
- GitHub-native integration
- Industry best practice for 2025+

**Tradeoffs Accepted:**
- Requires manual setup on pypi.org
- Only works from GitHub Actions (not local)

**Revisit If:**
- Need to publish from other CI systems
- PyPI changes OIDC requirements

---

## ADL-20260125-001

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Per-provider rate limiting via semaphores for parallel AI agent execution

**Chosen Option:** `RateLimitedParallelExecutor` with asyncio semaphores per provider (Anthropic: 5, OpenAI: 8 concurrent)

**Rejected Options:**
- Global rate limiter (doesn't account for different provider limits)
- Token bucket algorithm (more complex, same outcome)
- No rate limiting (429 errors under load)

**Why:**
- Different providers have different rate limits (Anthropic ~60 RPM, OpenAI ~90 RPM)
- Semaphores are simple, native to asyncio, and predictable
- Conservative defaults (5/8) leave headroom for retries
- Provider auto-detection from task metadata reduces boilerplate

**Tradeoffs Accepted:**
- Rate limits must be tuned per deployment
- Only effective with asyncio strategy (threading falls back to parent)
- No dynamic adjustment based on 429 responses

**Revisit If:**
- Need adaptive rate limiting based on actual API responses
- Providers change rate limits significantly
- Need cross-process rate limiting (current is per-process)

---

## ADL-20260125-002

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Native async for AI providers instead of executor-wrapped sync

**Chosen Option:** `AsyncAnthropic` and `AsyncOpenAI` clients with native `complete_async()` methods

**Rejected Options:**
- Always wrap sync in `run_in_executor` (wastes thread pool)
- Sync-only API (blocks event loop)
- Third-party async wrappers

**Why:**
- Native async is more efficient (no thread overhead)
- Official SDKs support async (`AsyncAnthropic`, `AsyncOpenAI`)
- Base class provides fallback for providers without native async
- Enables proper concurrent execution with rate limiting

**Tradeoffs Accepted:**
- Must maintain both sync and async code paths
- SDK version dependency (need recent versions)

**Revisit If:**
- SDK async support becomes unreliable
- Need to support providers without async SDKs

---

## ADL-20260125-003

**Project:** Gorgon
**Decision Class:** ARCH

**Decision:** Adapter pattern for incremental deprecation migration

**Chosen Option:** `WorkflowEngineAdapter` that wraps `WorkflowExecutor` with old `WorkflowEngine` interface

**Rejected Options:**
- Big-bang migration (risky, all-or-nothing)
- Maintain two parallel implementations (duplicate effort)
- Force all callers to update simultaneously (breaks existing code)

**Why:**
- Allows incremental migration - one file at a time
- Same interface means minimal code changes at call sites
- Adapter handles format conversion internally
- Deprecation warnings guide migration without breaking builds

**Tradeoffs Accepted:**
- Extra abstraction layer (minor overhead)
- Two object models coexist temporarily
- Must maintain adapter until full migration

**Revisit If:**
- All usages migrated to WorkflowExecutor (delete adapter)
- Object models diverge significantly

---

## ADL-20260126-001

**Project:** Argus Overview
**Decision Class:** SCOPE

**Decision:** Remove broadcast hotkeys feature for EVE Online EULA compliance

**Chosen Option:** Complete removal of input broadcasting functionality

**Rejected Options:**
- Keep feature with disclaimer (still violates EULA)
- Add EVE detection to disable feature (complexity, still shipped banned code)
- Make it opt-in/hidden (doesn't change EULA violation)

**Why:**
- CCP banned input broadcasting in January 2015 - permaban offense
- Feature sent keystrokes to multiple EVE windows simultaneously
- Even "fleet broadcast" framing doesn't change the technical violation
- One keypress = one action is the rule; one keypress = N actions is banned

**What Was Removed:**
- `send_key_to_window()` and `broadcast_key()` in window_capture_threaded.py
- `_register_broadcast_hotkeys()` and `_broadcast_key()` in main_window_v21.py
- Broadcast UI section in hotkeys_tab.py
- ~1159 lines of code and tests
- All documentation references

**Tradeoffs Accepted:**
- Lost feature that some users may have wanted
- Existing user settings have stale `broadcast_hotkeys` key (harmless)

**Revisit If:**
- CCP changes policy (unlikely)
- Never - this was a clear EULA violation

**Reference:**
- https://www.eveonline.com/news/view/input-broadcasting-and-multiplexing-policy
- https://support.eveonline.com/hc/en-us/articles/204873262

---

## ADL-20260127-001

**Project:** Chefwise
**Decision Class:** TOOLING

**Decision:** Migrate to Next.js 15 (not 16) as intentional stepping stone

**Chosen Option:** Next.js 15 + React 19 + ESLint 9

**Rejected Options:**
- Next.js 16 (deprecates Pages Router — requires App Router migration)
- Stay on Next.js 14 (accumulating tech debt, ESLint version conflicts)

**Why:**
- v15 fully supports Pages Router — zero code changes needed
- Resolves ESLint 8/9 version conflict that was causing CI issues
- Gets React 19 benefits (compiler improvements, performance)
- Sets up clean path for future App Router migration if needed

**Tradeoffs Accepted:**
- `next lint` deprecated in v15 (still works, needs CLI migration before v16)
- Will eventually need App Router migration for v16+

**Revisit If:**
- App Router migration becomes necessary for new features
- Next.js 16 adds must-have capabilities

---

## ADL-20260127-001

**Project:** Gorgon
**Decision Class:** SCOPE

**Decision:** Split large PRs (>2000 lines) into focused PRs by cherry-picking commits

**Chosen Option:** Close original PR, cherry-pick into 3 new branches by feature area

**Rejected Options:**
- Merge as-is (too large for meaningful review)
- Squash into single commit per branch (loses commit history)
- Rebase-based split (more complex, same result)

**Why:**
- Cherry-pick is simplest for additive-only changes (no deletions)
- Each PR independently reviewable and testable
- Clear dependency chain (PR C depends on PR B)
- Merge conflicts are manageable when both sides are purely additive

**Revisit If:**
- PRs involve deletions/renames (cherry-pick conflicts get harder)
- Branch has >10 interdependent commits

---

## ADL-20260127-002

**Project:** Gorgon
**Decision Class:** SECURITY

**Decision:** Sanitize prompt template variables by escaping curly braces before `str.format()` interpolation

**Chosen Option:** `text.replace("{", "{{").replace("}", "}}")` + 50KB length limit

**Rejected Options:**
- `string.Template` ($var syntax) — would require changing all existing templates
- Regex-based stripping — risks removing legitimate content
- No sanitization — leaves format string injection open

**Why:**
- `str.format()` interprets `{__class__}` etc. as attribute access
- Escaping is minimal, non-destructive, and backward-compatible
- Length limit prevents prompt stuffing attacks
- Applied at `PromptTemplate.format()` level — single enforcement point

**Revisit If:**
- Templates need nested formatting (double-brace escaping conflicts)
- Move to Jinja2 or another template engine

---

## ADL-20260127-003

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** Name-based dedup with platform priority for unified roster merging

**Chosen Option:** Case-insensitive name matching, ESPN > Sleeper > Yahoo priority, fill missing IDs from lower-priority sources

**Rejected Options:**
- ID-based matching across platforms (requires maintained cross-platform ID map)
- Manual-only linking (too much user friction)
- First-seen-wins (loses data from higher-quality sources)

**Why:**
- Player names are consistent enough across platforms for fantasy rosters
- ESPN has best projected points data, Sleeper has best injury data
- Merging fills gaps: one player gets ESPN ID + Sleeper ID + Yahoo ID
- Manual overrides handle the rare name mismatch

**Tradeoffs Accepted:**
- Name collisions possible (rare: "Josh Allen" QB vs "Josh Allen" DE)
- Case/accent differences could cause missed matches
- No persistent cross-platform ID mapping

**Revisit If:**
- Name collisions become a real problem (add position + team disambiguation)
- A cross-platform player ID service becomes available

---

## ADL-20260127-004

**Project:** GameSpace
**Decision Class:** ARCH

**Decision:** In-memory accuracy tracker with DB-backed decision retrieval

**Chosen Option:** `AccuracyTracker` stores outcomes in memory, queries decisions from PostgreSQL at metrics time

**Rejected Options:**
- Fully DB-backed outcomes (adds migration/schema work)
- Fully in-memory (loses data on restart)
- Redis-backed (another dependency for simple key-value)

**Why:**
- Outcomes are low-volume (users record after games end)
- Decision history already in PostgreSQL — no duplication
- In-memory is sufficient for portfolio demo
- Easy to swap to DB-backed later (just change `record_outcome` + `get_outcome`)

**Tradeoffs Accepted:**
- Outcomes lost on server restart (acceptable for demo)
- No persistence layer for outcomes yet

**Revisit If:**
- Moving to production with real users
- Need outcome data to survive deployments

---

<!-- New entries go below -->
